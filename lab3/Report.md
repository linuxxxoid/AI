# Отчет по лабораторной работе "Генерация последовательностей"

> Оценка: 5
> Дата проверки: 28.05.2020

#### Выполнила: Вельтман Л.Я.
#### Группа: М8О-307Б-17

Номер в группе: 7, Вариант: 1 ((остаток от деления (7-1) на 6)+1)

### Цель работы

Научиться генерировать последовательности с помощью рекуррентных нейронных сетей. В качестве последовательностей выступает: проза на русском языке, элемент последовательности - один символ. Источник данных - тексты на http://lib.ru

### Используемые входные данные

В качестве входных данных выступило произведение А.Грина "Алые паруса", доступное по ссылке http://lib.ru/RUSSLIT/GRIN/parusa.txt

### Предварительная обработка входных данных

Считанный текст разделим на части по заданному количеству символов (maxlen) с фиксированным шагом (step).
Чем больше maxlen, тем более "глубокими" становятся зависимости последующих символов от предыдущих. Также, чем меньше step, тем больше различных комбинаций символов попадет в обучающие данные.

```
maxlen = 40
step = 3
sentences = []
next_chars = []
for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i + maxlen])
    next_chars.append(text[i + maxlen])
print('nb sequences:', len(sentences))
```

Преобразуем полученные последовательности символов в последовательности векторов, в которых каждому символу будет соотвествовать вектор [x1, x2, ..., xK, ... xN], где xK = 1, если K равно индексу данного символа, N = количество в нашем наборе chars, а все остальные значения равны нулю.

```
x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        x[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1
```

### Эксперимент 1: Обычная полносвязная RNN

#### Архитектура сети

Приведу архитектуру сети RNN в виде последовательности слоёв. Определяем один скрытый слой LSTM с option['size'] = 256 единицами памяти. Для регуляции сети будем использовать слой Dropout с вероятностью 20, как правило, это делает узлы более устойчивыми к входам. Выходной уровень - это полносвязный (Dense) уровень, использующий функцию активации softmax для вывода прогнозирования вероятности для каждого из 51 символов в диапазоне от 0 до 1.
```
model1 = Sequential()
model1.add(SimpleRNN(option['size'], input_shape = (maxlen, vocab_size)))
model1.add(Dense(vocab_size)) 
model1.add(Activation('softmax'))
```

```
model1.summary()

Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
simple_rnn_2 (SimpleRNN)     (None, 128)               23040     
_________________________________________________________________
dense_2 (Dense)              (None, 51)                6579      
_________________________________________________________________
activation_2 (Activation)    (None, 51)                0         
=================================================================
Total params: 29,619
Trainable params: 29,619
Non-trainable params: 0
```

#### Результат

[Обучение](train_log/log_rnn.txt)

График потерь во время обучения нейросети:

![Иллюстрация к проекту](https://github.com/MAILabs-Edu-AI/lab-sequence-generation-henntaii/tree/master/images/rnn.png)

```
test_acc = model1.evaluate(x, y, verbose = 2)
print("Потери на данных")
print("%s: %.3f" % (model1.metrics_names[0], test_acc)) # loss (потери)
```
Потери на данных (по уже обученной нейросети)

loss: 1.585

Получившийся результат генерации:

```
секрета",  он несколько  минут  стоял
не
йло табки  болигами чно 
разбльсто видь,  коперна.
     -- смерти;  зналенью. в  хотял в --    т
да села  илострым лажев пресмирела  в  скопруглась ех дешкапи постоялись неся жи не  в
пелатил с ватром лиговоровая, дим рачь...  учажпывий
житнели рабряк меня,  выле на солокным клое, свовительогла надих лавшеми, грубыва  пратренкуй  е слотрел
          -- скавшко и дергул с  пробудама: "трежно и  всегдему.
          и замут марели осканома сколой лознояструшу  свленике   не раздым:  и мытарже рассказальнояв, нолск ит ед
ты овипять  тим локом
рющах нак "зочновая сь казы  выреблнуга.
     я эти --  хвотием и  мое пеже эторы, как  в  мотен крестый помдатрод.  и  лона,  лазьа дей оназнарыва всловае стромым в злантем   --  напрачава на половолик  дыхоль,  крокапип н узохдал   пе ер
емени надоволти. не  вожновил я свея, берить вытель
годно  астоеви спятвилогна встед щнеп,  одрадтого, водвровсем а  рнам-мажелся.   полого вывая  на застослеля,  и  вескизани!  скахом, весвинь  но зачтоме в е за черол  к же,
дожа.
--
стьюбемы
  пеланием,  котерого своротя...
потем, сводать, полутал  бы   мотре, чегорню
м свыралала, его плоголус вепих,   эта я теня очно  мичня бя мастверка; одрогодвотвли чистель эти    замонать, апечудать  елласто  не все стала призблля, выто краженные, асецом.
     нансмостровоя иси там чамыв мезан
и злачто и всего  подухлать дум что  все жном, везел ок  ничали  сем,  топры
не прапиланный мобчер,  спой делся, врость всказалась вукились охменечно
сылего, понорах меде  вуть
```

#### Вывод по данному эксперименту

Потери в сети уменьшались почти каждую эпоху, поэтому, я решила, что сеть сможет показать себя в лучшем свете при увеличении эпох, поэтому поставила значение отвечающее за их количество = 120, но, начиная с 117 эпохи значения начали переставать улучшаться и извлечь какую-либо выгоду из обучения уже не оставалось возможным. Можно отметить некоторые замечания по поводу сгенерированного текста с использованием загруженной модели RNN. Заметны лишь несколько настоящих русских слов, весь текст лишен всяческого смысла. Но в некоторых местах можно увидеть попытки правильного соблюдения пунктуации. Например, прямая речь: 
```
-- скавшко и дергул с  пробудама: "трежно и  всегдему.
          и замут марели осканома сколой лознояструшу  свленике   не раздым:  и мытарже рассказальнояв, нолск ит ед
ты овипять  тим локом
рющах нак "
```
Из-за того, что сам по себе русский язык сложен и многообразен (наличие падежей и родов, от которых зависят окончания слов), то RNN дает отнюдь не самый хороший результат. Возможно обучение сети на произведении, написанном на английском языке дало бы лучший результат.

### Эксперимент 2: Однослойный LSTM

#### Архитектура сети

Приведу архитектуру однослойной LSTM в виде последовательности слоёв. Определяем один скрытый слой LSTM с option['size'] = 128 единицами памяти. Для регуляции сети будем использовать слой Dropout с вероятностью 20, как правило, это делает узлы более устойчивыми к входам. Выходной уровень - это полносвязный (Dense) уровень, использующий функцию активации softmax для вывода прогнозирования вероятности для каждого из 51 символов в диапазоне от 0 до 1.

```
    model2 = Sequential()
    model2.add(LSTM(option['size'], input_shape = (maxlen, vocab_size)))
    model2.add(Dropout(0.2))
    model2.add(Dense(vocab_size))
    model2.add(Activation('softmax'))
```

```
model2.summary()

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 128)               92160     
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 51)                6579      
_________________________________________________________________
activation_1 (Activation)    (None, 51)                0         
=================================================================
Total params: 98,739
Trainable params: 98,739
Non-trainable params: 0
_________________________________________________________________
```

#### Результат

[Обучение](train_log/log_lstm1.txt)

График потерь во время обучения нейросети:

![Иллюстрация к проекту](https://github.com/MAILabs-Edu-AI/lab-sequence-generation-henntaii/tree/master/images/lstm1.png)

```
test_acc = model2.evaluate(x, y, verbose = 2)
print("Потери на данных")
print("%s: %.3f" % (model2.metrics_names[0], test_acc)) # loss (потери)
```

Потери на данных (по уже обученной нейросети)

loss: 0.694

Получившийся результат генерации:

```
секрета",  он несколько  минут  стоял
не
йло табки  болигами чно 
разбльсто видь,  коперна.
     -- смерти;  зналенью. в  хотял в --    т
да села  илострым лажев пресмирела  в  скопруглась ех дешкапи постоялись неся жи не  в
пелатил с ватром лиговоровая, дим рачь...  учажпывий
житнели рабряк меня,  выле на солокным клое, свовительогла надих лавшеми, грубыва  пратренкуй  е слотрел
          -- скавшко и дергул с  пробудама: "трежно и  всегдему.
          и замут марели осканома сколой лознояструшу  свленике   не раздым:  и мытарже рассказальнояв, нолск ит ед
ты овипять  тим локом
рющах нак "зочновая сь казы  выреблнуга.
     я эти --  хвотием и  мое пеже эторы, как  в  мотен крестый помдатрод.  и  лона,  лазьа дей оназнарыва всловае стромым в злантем   --  напрачава на половолик  дыхоль,  крокапип н узохдал   пе ер
емени надоволти. не  вожновил я свея, берить вытель
годно  астоеви спятвилогна встед щнеп,  одрадтого, водвровсем а  рнам-мажелся.   полого вывая  на застослеля,  и  вескизани!  скахом, весвинь  но зачтоме в е за черол  к же,
дожа.
--
стьюбемы
  пеланием,  котерого своротя...
потем, сводать, полутал  бы   мотре, чегорню
м свыралала, его плоголус вепих,   эта я теня очно  мичня бя мастверка; одрогодвотвли чистель эти    замонать, апечудать  елласто  не все стала призблля, выто краженные, асецом.
     нансмостровоя иси там чамыв мезан
и злачто и всего  подухлать дум что  все жном, везел ок  ничали  сем,  топры
не прапиланный мобчер,  спой делся, врость всказалась вукились охменечно
сылего, понорах меде  вуть
```

#### Вывод по данному эксперименту

Приближаясь уже к 100 эпохе потери перестали уменьшаться на значимое количество единиц, но все же, по сравнению, с сетью RNN, после 120 эпохи дало результат = 0.9410, что меньше результата потерь у RNN на 120 эпохе ( = 1.6220) на 0.681. Изучая сгенерированный текст, я заметила, что символы разделены на словесные группы и стало намного больше настоящих русский слов. Некоторые слова в последовательности имеют смысл. В целом, текст смотрится более гармонично, чем предыдущий. У меня создалось впечатление, что я читаю какой-то старославянский текст. 

### Эксперимент 3: Двухслойный LSTM

#### Архитектура сети

Приведем архитектуру двухслойной LSTM сети в виде последовательности слоёв. Определим количество узлов для слоя LSTM, равное 256, и добавим еще второй слой LSTM. Теперь мы имеем уже два слоя LSTM, регулязацию которых выполняем при помощи техники dropout. Добавляем полносвязный слой с функцией активации - непрерывная функция softmax с 51 выходами, равное количеству уникальных символов в нашем тексте.

 ``` 
    model3 = Sequential()
    model3.add(LSTM(option['size'], input_shape = (maxlen, vocab_size), return_sequences = True))
    model3.add(Dropout(0.2))
    model3.add(LSTM(option['size']))
    model3.add(Dropout(0.2))
    model3.add(Dense(vocab_size))
    model3.add(Activation('softmax'))
```
```
model3.summary()

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 40, 256)           315392    
_________________________________________________________________
dropout_1 (Dropout)          (None, 40, 256)           0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 256)               525312    
_________________________________________________________________
dropout_2 (Dropout)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 51)                13107     
_________________________________________________________________
activation_1 (Activation)    (None, 51)                0         
=================================================================
Total params: 853,811
Trainable params: 853,811
Non-trainable params: 0
_________________________________________________________________
```

#### Результат

[Обучение](train_log/log_lstm2.txt)

График потерь во время обучения нейросети:

![Иллюстрация к проекту](https://github.com/MAILabs-Edu-AI/lab-sequence-generation-henntaii/tree/master/images/lstm2.png)

```
test_acc = model3.evaluate(x, y, verbose = 2)
print("Потери на данных")
print("%s: %.3f" % (model3.metrics_names[0], test_acc)) # loss (потери)
```

Потери на данных (по уже обученной нейросети)

loss: 0.404

Получившийся результат генерации:

```
, кофе, шелк, ценные  породы
деревьев:  
своомом дего не этом
дуской угрещения такно в дух ем от поймой лютий корыбу, -- сказал лачти тех, но дне ханут". -- говорить, это без работ люс, --  сказал получный ребмем  материю с грействиеша, что бы на
колнее  спегнул,  зная
мушчымало  воскушивая,  заступил  разотренного уморать вотли, что его что- изнешелся  дея,  -- солнца нис дрягой и скородь. в тар.  но ей не тасал, стрепносли, закам в шумом, погля увышат по пещах, но спеящий горовый состоя. поддянногу, следанная, кольбо  его игрушке, как  самом и.
вышь  грузим, как все будной, на
презхотов  она не  быстяты
есть человеки  и  заототой  приверенность. наде желивые  постранного востренно.
     девоваю мучки его как бы  продне негу, больший пустор, свостой по  хорим  задочил.
     -- под грэй меннерса закам в отверимом. ответил в
зылкение -- как  капитны  палую
жиро, и
грубь,   придолжал  перусть ег
  дошан быс
торотой и  к зыбы,  поплещива:  он  рошел  прочил  на  бустой дела. -- скрывал гольним, каковой постринно стерковую грусом, парание рук  с внеми
асольной  крапки,  он  улиженный  цвет   своем  прохозол  зналось  и
 томоросили, напередав своет нивого были какаянти сдоят литова соверей в очерь. водег  сероца, делавшей соберательно, когда на палуха,  за  викурничалась чтобы вышула и теперого  и  дешь
к    грэй  значественной  млестном  таммор  двох  спряви
гороя..
     -- ну.
не  он  пробрал,  встьу,  ассоль  прошилавая  солнательно,  как  в  головой долого. -- "дерьки! -- это жаниты сдовался, заспел ловитно  и
негруб
```

#### Вывод по данному эксперименту

По сравнению с полученными результатами у предыдущей сети, очевидно, что улучшилось качество сгенерированного текста. Время обучения также занимает большее количество времени нежели обучение однослойной нейросети LSTM, но при этом значение потерь улучшается быстрее, поэтому необходимость в количестве эпох = 120 отпадает, теперь я использую 30 эпох. Значение потерь равно = 0.404, что говорит о качественном обучении нейросети по сравнению с предыдущими. Судя по полученному сгенерированному тексту, он улучшился, орфографических ошибок меньше, количество осмысленных русских слов тоже увеличилось, и сам текст выглядит более реалистичным, но все же совершенно бессмысленным. Если бы элементом последовательности выступало слово, а не как в моем случае - символ, мне кажется, что результаты были бы лучше.

### Эксперимент 4: Однослойный GRU

#### Архитектура сети

Архитектура ондослойной GRU сети: количество узлов равно 256 для слоя GRU. Добавим слой Flatten для изменения формы тензора. Во избежание переобучения добавим Dropout. Затем добавляем полносвязный слой с функцией активации - непрерывная функция softmax с 51 выходами, равное количеству уникальных символов в нашем тексте.

```
model4 = Sequential()
model4.add(GRU(option['size'], input_shape = (maxlen, vocab_size), return_sequences = True))
model4.add(Flatten())
model4.add(Dropout(0.2))
model4.add(Dense(vocab_size)) 
model4.add(Activation('softmax'))
```

```
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
gru_1 (GRU)                  (None, 40, 256)           236544    
_________________________________________________________________
flatten_1 (Flatten)          (None, 10240)             0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 10240)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 51)                522291    
_________________________________________________________________
activation_1 (Activation)    (None, 51)                0         
=================================================================
Total params: 758,835
Trainable params: 758,835
Non-trainable params: 0
```

#### Результат

[Обучение](train_log/log_gru.txt)

График потерь во время обучения нейросети:

![Иллюстрация к проекту](https://github.com/MAILabs-Edu-AI/lab-sequence-generation-henntaii/tree/master/images/gru.png)

```
test_acc = model4.evaluate(x, y, verbose = 2)
print("Потери на данных")
print("%s: %.3f" % (model4.metrics_names[0], test_acc)) # loss (потери)
```

Потери на данных (по уже обученной нейросети)

loss: 0.116

Получившийся результат генерации:

```
ась бы прочь,
заплакав и изнемогая от ст
акимысай, и плосазав с варе пропалные вые  присловимы он  начартени нижиса  собе  глазвы  другах,
носулы, пакшино я  син   ону промола  одака и  судавот  нам
дитью.  асколецноя   на  -- страчать, тек  удерня,  забыталенно  котоесми  демомму семеенио  сморти, он накидужды нице у
весто, капирами  ее  прасневе  топке и  образны и родакрыт ю  пряцстона  жизниси,  не водой в житению, не васивая  полисе  в  с ротем  что  ето  толение
далалосо  то вертю дото неспинниралист гру
слоскались бы о
на  крапите,  припола  оборти  огомого  истороу.  кого  соляцо сказал уда рым
и дему,  разилась  кам на примототомости
го и
белать.. корди выдыла и трактые   прастая, чаот  как  гомотри  слада.  в дола,  спридевала  улев  с  удолом 
черо.  эти  скозотом, козоровый   на из делег    и  урола  лодской,  ратчки  за петер   кручка.
     енны  в деланное  молчеги  он полловала  и дуже,  бел в зомолот
ин  отнетел в нествых дал к гда, -- жезнам  слушки
слажа, пятая по мерты невруками ее в что берега  обник, стала
сказальном,  останоми  и остана.  усыло   снажи не вуюбыто теска в тамты  волеш,  са
заминнеши ии мести не разгудиймот и которой  двимно облакрели, в отказал а
его  изных нем вемхода как  и тодко  греза иледьное
соддеставием ожись сорон прянем. никомо белетно огразной привывали ос  кревовом огаза.
      --  что  мала оно котрача.  пошаз легох, акой учистоем витр  отой вдехние,  как рыжи од
могодольное, прищення  в родпя,  стое,  застарива  зное порлибовались  блестысалько теваки и долости игол
```

#### Вывод по данному эксперименту

Полученные результаты мне нравятся больше всего, так как значение потери равно 0.116 - это лучший полученный результат. Мне показалось, что пунктуация в тексте приобрела новый уровень. Количество русских слов увеличилось, но качество самого текста все же не на высоте, он все еще напоминает бессмысленный старославянский, но это связано с посимвольным предсказанием, если бы использовались целые слова, текст содержал бы в себе больше ценности.

### Выводы

Из преимуществ RNN хотелось бы отметить, что она хорошо подходит для построения языковых моделей как посимвольных так и пословных. Главный плюс RNN в том, что она способна эффективно использовать данные с предыдущих шагов. RNN принимает на вход последовательность входных данных (в нашем случае символов). А, например, сверточной сети (CNN), на вход подается целое изображение. Для RNN же входными данными может служить как короткое или длинное предложение, так и целое произведение из тысячи абзацев. Это и является самым явным отличием RNN от традиционной нейронной сети. Более того, порядок, в котором подаются данные, может влиять на то, как в процессе обучения меняются матрицы весов и векторы скрытых состояний. К концу обучения в векторах скрытых состояний должна накопиться информация из прошлых шагов.

Однако у этих рекуррентных нейронных сетей есть и недостаток. В каждый момент времени они должны хранить информацию о входных данных за многочисленные предыдущие интервалы времени, но на практике такие протяженные зависимости не поддаются обучению. Это связано с проблемой затухания градиента, напоминающего эффект, наблюдающийся в сетях прямого распространения с большим количеством слоев: по мере увеличения количества слоев сеть в конечном итоге становится необучаемой. 

Для решения этой проблемы был предложен слой долгой краткосрочной памяти (Long short-term memory, LSTM) который позволяет снова задействовать в процессе обучения предыдущую информацию и тем самым решить проблему затухания градиента. Слой управляемых рекуррентных блоков (Gated Recurrent Unit, GRU) основан на том же принципе, что и слой LSTM, однако рекуррентные блоки представляют собой более простые структуры по сравнению с LSTM и, соответственно, менее затратные в вычислительном смысле. В традиционных рекуррентных нейронных сетях для реализации обратной связи используется комбинация скрытого состояния на предыдущем шаге и текущих входных данных в слое с нелинейной функцией активации. В LSTM-сети обратная связь реализуется аналогично, но нейронных слоев используется не один, а четыре. 

GRU – это вариант LSTM слоя, также обладающий устойчивостью к проблеме затухания градиента, но структура его проще, а потому и обучается он быстрее. Вместо трех вентилей в ячейки LSTM – входного забывания и выходного, в ячейке GRU всего два вентиля: обновления и сброса. Вентиль обновления определяет, какую часть предыдущего запомненного значения сохранять, а вентиль сброса – как смешивать новый вход с предыдущей памятью. 

Оба эти метода разработаны для того, чтобы сохранять отдаленные зависимости в последовательностях слов (символов). Под отдаленными зависимостями имеются в виду такие ситуации, когда два слова или фразы могут встретиться на разных временных шагах, но отношения между ними важны для достижения конечной цели. LSTM и GRU отслеживают эти отношения с помощью фильтров, которые могут сохранять или сбрасывать информацию из обрабатываемой последовательности. Различие между двумя методами состоит в количестве фильтров (GRU – 2, LSTM – 3). Это влияет на количество нелинейностей, которое приходит от входных данных и в конечном итоге влияет на процесс вычислений. 

Исходя из моего опыта, GRU обучаются быстрее и работают лучше, чем LSTM. Время, потраченное на одну эпоху у GRU в среднем равно 66.68 секунд, двухслойной LSTM - 154.1 секунд. Теоретически LSTM должны запоминать более длинные последовательности, чем GRU, и превосходить их в задачах, требующих моделирования отношений на расстоянии (long-distance relations).

Также, LSTM и GRU действительно показали лучшие результаты, чем простая RNN. Изучив текст, который сгенерировала сеть RNN, и тексты сетей с LSTM- и GRU-слоями, то у последних они были лучше. Очевидно, что первая не справляется с долгосрочными зависимостями в тексте, а, как и предполагалось, последним удается их обработать.

Таким образом, можно сделать вывод, что выбор архитектуры нейронной сети тесно связан со спецификой решаемой задачи. Решая задачу, стоит перебрать разные подходы и гиперпараметры и выбрать среди них те, которые смогут уловить закономерности в данном наборе текстов. Однако, в целом рекуррентные нейронные сети показывают гораздо более высокие результаты, чем простые нейронные сети или другие классификаторы.
